//// Created by yijie.yu on 2019/1/28.//#ifndef CPP_REMOTE_DROPBLOCK_INL_H#define CPP_REMOTE_DROPBLOCK_INL_H#include <dmlc/logging.h>#include <dmlc/parameter.h>#include <mxnet/operator.h>#include <stdio.h>#include<stdlib.h>#include <math.h>#include <map>#include <cmath>#include <random>#include <vector>#include <string>#include <utility>#include <algorithm>#include "../mxnet_op.h"#include "../mshadow_op.h"#include "../random/sampler.h"#include "../tensor/elemwise_binary_broadcast_op.h"#if defined(USE_MKL) && defined(_OPENMP)#include <omp.h>#include <mkl_vml_functions.h>#include <mkl_vsl.h>#endif //CPP_REMOTE_DROPBLOCK_INL_Hnamespace dropblock {    enum DropblockOpInputs {kData};    enum DropblockOpOutputs {kOut, kMask};    enum DropblockOpForwardResource {kRandom};    enum DropblockOpMode {kTraining, kAlways};}  // namespace dropoutnamespace mxnet {    namespace op {#if defined(USE_MKL) && defined(_OPENMP)        static void bernoulli_generate(int n, double p, int* r) {  const int seed = 17 + rand() % 4096;  // NOLINT(runtime/threadsafe_fn)  const int nthr = engine::OpenMP::Get()->GetRecommendedOMPThreadCount();# pragma omp parallel num_threads(nthr)  {    const int ithr = omp_get_thread_num();    const int avg_amount = (n + nthr - 1) / nthr;    const int my_offset = ithr * avg_amount;    const int my_amount = std::min(my_offset + avg_amount, n) - my_offset;    if (my_amount > 0) {      VSLStreamStatePtr stream;      vslNewStream(&stream, VSL_BRNG_MCG31, seed);      vslSkipAheadStream(stream, my_offset);      viRngBernoulli(VSL_RNG_METHOD_BERNOULLI_ICDF, stream, my_amount,        r + my_offset, p);      vslDeleteStream(&stream);    }  }}#endif  // USE_MKL && _OPENMP        const int MAX_DIM = 5;        struct DropblockParam : public dmlc::Parameter<DropblockParam> {            real_t p;            int mode;            int block_size;            TShape axes;            DMLC_DECLARE_PARAMETER(DropblockParam) {                DMLC_DECLARE_FIELD(p).set_default(0.5)                        .set_range(0, 1)                        .describe("Fraction of the input that gets dropped out during training time.");                DMLC_DECLARE_FIELD(block_size).set_default(3)                        .describe("the block size.");                DMLC_DECLARE_FIELD(mode)                        .add_enum("training", dropblock::kTraining)                        .add_enum("always", dropblock::kAlways)                        .set_default(dropblock::kTraining)                        .describe("Whether to only turn on dropblock during training or to also turn on for inference.");                DMLC_DECLARE_FIELD(axes).set_default(TShape())                        .describe("Axes for variational dropblock kernel.");            }        };  // struct DropoutParam        template<typename xpu, typename DType>        class DropblockOp {        public:            /*!             * \brief Dropout kernel, compute dropout tensor             */            template <int req>            struct dropblock_backward_kernel{                MSHADOW_XINLINE static void Map( int id ,DType* in_grad,const DType* out_grad,const DType* out_data                        ){                    KERNEL_ASSIGN(in_grad[id],req,out_grad[id]*out_data[id]);                }            };            void Init(const DropblockParam &param) {                this->pkeep_ = 1.0f - param.p;                this->block_size_=param.block_size;                this->mode_ = static_cast<dropblock::DropblockOpMode>(param.mode);                this->axes_ = param.axes;            }            void Forward(const OpContext &ctx,                         const std::vector<TBlob> &in_data,                         const std::vector<OpReqType> &req,                         const std::vector<TBlob> &out_data) {                printf("进入了forward!\n");                if (req[dropblock::kOut] != kNullOp) {                    CHECK_EQ(in_data.size(), 1U);                    if (ctx.is_train) {                        CHECK_EQ(out_data.size(), 2U);                    }                    Stream<xpu> *s = ctx.get_stream<xpu>();                    const TBlob &out = out_data[dropblock::kOut];                    int nums=in_data[dropblock::kData].shape_[0];                    int channel=in_data[dropblock::kData].shape_[1];                    int height=in_data[dropblock::kData].shape_[2];                    int width=in_data[dropblock::kData].shape_[3];                    Shape<4> dshape=Shape4(nums,channel,height,width);                    Tensor<xpu, 4, DType> data_in = in_data[dropblock::kData].get_with_shape<xpu,4,DType>(dshape,s);                    Tensor<xpu, 4, DType> data_out = out_data[dropblock::kOut].get_with_shape<xpu,4,DType>(dshape,s);                    if (ctx.is_train || this->mode_ == dropblock::kAlways) {                        Tensor<xpu, 4, DType> data_mask = out_data[dropblock::kMask].get_with_shape<xpu,4,DType>(dshape,s);#if !defined(__CUDACC__) && defined(USE_MKL) && defined(_OPENMP)                        DType* outptr = data_out.dptr_;      DType* dataptr = data_in.dptr_;      auto maskptr = reinterpret_cast<int*>(data_mask.dptr_);      int count = data_mask.shape_[0]*data_mask.shape_[1]*data_mask.shape_[2]*data_mask.shape_[3];      bernoulli_generate(count, this->pkeep_, maskptr);      const float pk_1 = 1.0f / pkeep_;      #pragma omp parallel for num_threads(engine::OpenMP::Get()->GetRecommendedOMPThreadCount())      for (int i = 0; i < count; ++i) {        outptr[i] = dataptr[i] * maskptr[i] * pk_1;      }#else                        RandGenerator<xpu, DType> *pgen = ctx.requested[0].get_parallel_random<xpu, DType>();//get a random number generator                        CHECK_NOTNULL(pgen);                        real_t pkeep=this->pkeep_;                        const index_t N=out.Size();                        if (N <= 0) {                            return;                        }                        const index_t nloop = (N + RandGenerator<xpu>::kMinNumRandomPerThread - 1) /                                              RandGenerator<xpu>::kMinNumRandomPerThread;                        const index_t nthread = std::min(nloop,                                                         static_cast<index_t>(RandGenerator<xpu>::kNumRandomStates));                        const index_t step = (N + nthread - 1) / nthread;                        index_t block_size=this->block_size_;                        printf("进入了计算步骤 block_size为:%lld\n",block_size);                        const index_t start = nthread * step;                        printf("start的值的大小是：%lld\n",start);                        const index_t end = start + step;                        printf("end的值的大小是:%lld\n",end);                        //typename RandGenerator<xpu, DType>::Impl genImpl(&pgen, nthread);                        index_t feat_size = height;//9                        //0.092                        double gamma = ((1 - pkeep) / (block_size * block_size)) * ((feat_size * feat_size) /                                                                                    ((feat_size - block_size + 1) *                                                                                     (feat_size - block_size + 1)));                        printf("gamma为:%lf\n",gamma);                        index_t mask_reduction = block_size / 2;//1                        index_t mask_height, mask_width;                        if ((block_size % 2) != 0) {                            mask_height = height - mask_reduction * 2;//7                            mask_width = width - mask_reduction * 2;//7                        } else {                            mask_height = height - mask_reduction * 2 + 1;                            mask_width = width - mask_reduction * 2 + 1;                        }                        index_t mask_area = mask_height * mask_width;//49                        index_t n = static_cast<int>(mask_area * gamma);//4.5                        //实现np.arange()操作                        std::vector<int> a;                        for (int j = 0; j < mask_area; ++j) {                            a.push_back(j);                        }                        //声明动态三维数组                        std::vector<std::vector<std::vector<int>>> mask(nums, std::vector<std::vector<int >>(1,std::vector<int>(mask_area,                                                                                                                                     0)));                        //实现random.sample(a,n)的操作                        int l = 0;                        while (l < n) {                            index_t randnum = rand() % mask_area;//得到不大于mask_area的随机数                            if (a[randnum] != -100) {                                a[randnum] = -100;                                ++l;                            }                        }                        printf("mask:\n");                        for (int j = 0; j < nums; ++j) {                            for (int k = 0; k < mask_area; ++k) {                                if (a[k] == -100) {                                    mask[j][0][k] = 1;                                }                                printf("%d\t",mask[j][0][k]);                            }                            printf("\n");                        }                        std::vector<std::vector<std::vector<std::vector<int>>>> mask_new(nums,                                                                                         std::vector<std::vector<std::vector<int>>>(                                                                                                 1,                                                                                                 std::vector<std::vector<int>>(                                                                                                         mask_height,                                                                                                         std::vector<int>(                                                                                                                 mask_width))));//初始化维度大小为n*m*x*y                        index_t mask_i = 0;                        index_t mask_j = 0;                        //对应 mask=mask.reshape([data.shape[0], 1, mask_height, mask_width])                        printf("mask_new:\n");                        for (int m = 0; m < nums; ++m) {                            for (int n = 0; n < 1; ++n) {                                for (int l = 0; l < mask_area; ++l) {                                    mask_i = l / mask_width;                                    mask_j = l % mask_width;                                    mask_new[m][n][mask_i][mask_j] = mask[m][n][l];                                    printf("%d\t",mask_new[m][n][mask_i][mask_j]);                                }                                printf("\n");                            }                            printf("\n");                        }                        //生成卷积所使用的weight_mat                        std::vector<std::vector<std::vector<std::vector<int>>>> weight_mat(nums,                                                                                           std::vector<std::vector<std::vector<int>>>(                                                                                                   1,                                                                                                   std::vector<std::vector<int>>(                                                                                                           block_size,                                                                                                           std::vector<int>(                                                                                                                   block_size,                                                                                                                   1))));//初始化维度大小为n*m*x*y                        //卷积前的padding操作                        //根据block_size的不同选择不同的padding策略                        index_t  padding=0;                        if(block_size==3)                        {                            padding=block_size/2 +1;                        }                        else if (block_size==5)                        {                            padding=ceil(block_size/2.0)+1;                        }                        else if(block_size>5)                        {                            padding=ceil(block_size/2.0)+2;                        }                        //index_t padding = ceil(block_size / 2.0) + 2;//2                        index_t padding_height = mask_height + 2 * padding;//7+4                        index_t padding_width = mask_width + 2 * padding;//11                        std::vector<std::vector<std::vector<std::vector<int>>>> mask_padding(nums,                                                                                             std::vector<std::vector<std::vector<int>>>(                                                                                                     1,                                                                                                     std::vector<std::vector<int>>(                                                                                                             padding_height,                                                                                                             std::vector<int>(                                                                                                                     padding_width))));                        printf("mask_padding:\n");                        for (int n = 0; n < nums; ++n) {                            for (int j = 0; j < 1; ++j) {                                for (int k = 0; k < padding_height; ++k) {                                    for (int m = 0; m < padding_width; ++m) {                                        if (k < padding || m < padding) {                                            mask_padding[n][j][k][m] = 0;                                        } else if (k > (mask_height + 1) || m > (mask_width + 1)) {                                            mask_padding[n][j][k][m] = 0;                                        } else {                                            mask_padding[n][j][k][m] = mask_new[n][j][k - padding][m - padding];                                        }                                        printf("%d\t",mask_padding[n][j][k][m]);                                    }                                    printf("\n");                                }                                printf("\n");                            }                            printf("\n");                        }                        std::vector<std::vector<std::vector<int >>> mask_1d(nums, std::vector<std::vector<int >>(1,                                                                                                                      std::vector<int >(                                                                                                                              padding_height *                                                                                                                              padding_width)));                        //把mask_padding平铺成三维数组，把卷积核平铺为一维数组,默认mask_height==mask_width                        for (int n = 0; n < nums; ++n) {                            for (int j = 0; j < 1; ++j) {                                for (int k = 0; k < padding_height; ++k) {                                    for (int m = 0; m < padding_width; ++m) {                                        mask_1d[n][j][m + k * padding_width] = mask_padding[n][j][k][m];                                    }                                }                            }                        }                        //把kernel平铺为一维数组                        std::vector<std::vector<std::vector<int>>> kernel_1d(nums, std::vector<std::vector<int>>(1,                                                                                                                      std::vector<int>(                                                                                                                              block_size *                                                                                                                              block_size)));                        printf("kernel_1d:\n");                        for (int n = 0; n < nums; ++n) {                            for (int j = 0; j < 1; ++j) {                                for (int k = 0; k < block_size; ++k) {                                    for (int m = 0; m < block_size; ++m) {                                        kernel_1d[n][j][m + k * block_size] = weight_mat[n][j][k][m];                                        printf("%d\t",kernel_1d[n][j][m + k * block_size]);                                    }                                    printf("\n");                                }                                printf("\n");                            }                            printf("\n");                        }                        //计算卷积输出矩阵的维数                        index_t outm = padding_height - block_size + 1;//9                        //计算卷积过程中的被卷积矩阵的宽和高                        index_t convAw = block_size * block_size;//9                        index_t convAh = padding_height * padding_width;//121                        //定义一个卷积过程中的矩阵                        std::vector<std::vector<std::vector<int>>> A_convert(nums, std::vector<std::vector<int>>(1,                                                                                                                      std::vector<int>(                                                                                                                              convAh *                                                                                                                              convAw)));                        for (int n = 0; n < nums; ++n) {                            for (int j = 0; j < 1; ++j) {                                for (int k = 0; k < outm; ++k) {                                    for (int m = 0; m < outm; ++m) {                                        index_t wh = k * outm * convAw + m * convAw;//k*9*9+m*121                                        index_t col1 = k * padding_height + m;//k*11+m  0                                        index_t col2 = (k + 1) * padding_height + m;//(k+1)*11+m 11                                        index_t col3 = (k + 2) * padding_height + m;//(k+2)*11+m  22                                        index_t col4 = (k + 3) * padding_height + m;//(k+3)*11+m                                        index_t col5 = (k + 4) * padding_height + m;//(k+4)*11+m                                        index_t col6 = (k + 5) * padding_height + m;                                        index_t col7 = (k + 6) * padding_height + m;                                        if (block_size == 3) {                                            A_convert[n][j][wh] = mask_1d[n][j][col1];                                            A_convert[n][j][wh + 1] = mask_1d[n][j][col1 + 1];                                            A_convert[n][j][wh + 2] = mask_1d[n][j][col1 + 2];                                            A_convert[n][j][wh + 3] = mask_1d[n][j][col2];                                            A_convert[n][j][wh + 4] = mask_1d[n][j][col2 + 1];                                            A_convert[n][j][wh + 5] = mask_1d[n][j][col2 + 2];                                            A_convert[n][j][wh + 6] = mask_1d[n][j][col3];                                            A_convert[n][j][wh + 7] = mask_1d[n][j][col3 + 1];                                            A_convert[n][j][wh + 8] = mask_1d[n][j][col3 + 2];                                        } else if (block_size == 5) {                                            A_convert[n][j][wh] = mask_1d[n][j][col1];                                            A_convert[n][j][wh + 1] = mask_1d[n][j][col1 + 1];                                            A_convert[n][j][wh + 2] = mask_1d[n][j][col1 + 2];                                            A_convert[n][j][wh + 3] = mask_1d[n][j][col1 + 3];                                            A_convert[n][j][wh + 4] = mask_1d[n][j][col1 + 4];                                            A_convert[n][j][wh + 5] = mask_1d[n][j][col2];                                            A_convert[n][j][wh + 6] = mask_1d[n][j][col2 + 1];                                            A_convert[n][j][wh + 7] = mask_1d[n][j][col2 + 2];                                            A_convert[n][j][wh + 8] = mask_1d[n][j][col2 + 3];                                            A_convert[n][j][wh + 9] = mask_1d[n][j][col2 + 4];                                            A_convert[n][j][wh + 10] = mask_1d[n][j][col3];                                            A_convert[n][j][wh + 11] = mask_1d[n][j][col3 + 1];                                            A_convert[n][j][wh + 12] = mask_1d[n][j][col3 + 2];                                            A_convert[n][j][wh + 13] = mask_1d[n][j][col3 + 3];                                            A_convert[n][j][wh + 14] = mask_1d[n][j][col3 + 4];                                            A_convert[n][j][wh + 15] = mask_1d[n][j][col4];                                            A_convert[n][j][wh + 16] = mask_1d[n][j][col4 + 1];                                            A_convert[n][j][wh + 17] = mask_1d[n][j][col4 + 2];                                            A_convert[n][j][wh + 18] = mask_1d[n][j][col4 + 3];                                            A_convert[n][j][wh + 19] = mask_1d[n][j][col4 + 4];                                            A_convert[n][j][wh + 20] = mask_1d[n][j][col5];                                            A_convert[n][j][wh + 21] = mask_1d[n][j][col5 + 1];                                            A_convert[n][j][wh + 22] = mask_1d[n][j][col5 + 2];                                            A_convert[n][j][wh + 23] = mask_1d[n][j][col5 + 3];                                            A_convert[n][j][wh + 24] = mask_1d[n][j][col5 + 4];                                        }else if  (block_size == 7) {                                            A_convert[n][j][wh] = mask_1d[n][j][col1];                                            A_convert[n][j][wh + 1] = mask_1d[n][j][col1 + 1];                                            A_convert[n][j][wh + 2] = mask_1d[n][j][col1 + 2];                                            A_convert[n][j][wh + 3] = mask_1d[n][j][col1 + 3];                                            A_convert[n][j][wh + 4] = mask_1d[n][j][col1 + 4];                                            A_convert[n][j][wh + 5] = mask_1d[n][j][col1 + 5];                                            A_convert[n][j][wh + 6] = mask_1d[n][j][col1 + 6];                                            A_convert[n][j][wh + 7] = mask_1d[n][j][col2];                                            A_convert[n][j][wh + 8] = mask_1d[n][j][col2 + 1];                                            A_convert[n][j][wh + 9] = mask_1d[n][j][col2 + 2];                                            A_convert[n][j][wh + 10] = mask_1d[n][j][col2 + 3];                                            A_convert[n][j][wh + 11] = mask_1d[n][j][col2 + 4];                                            A_convert[n][j][wh + 12] = mask_1d[n][j][col2 + 5];                                            A_convert[n][j][wh + 13] = mask_1d[n][j][col2 + 6];                                            A_convert[n][j][wh + 14] = mask_1d[n][j][col3];                                            A_convert[n][j][wh + 15] = mask_1d[n][j][col3 + 1];                                            A_convert[n][j][wh + 16] = mask_1d[n][j][col3 + 2];                                            A_convert[n][j][wh + 17] = mask_1d[n][j][col3 + 3];                                            A_convert[n][j][wh + 18] = mask_1d[n][j][col3 + 4];                                            A_convert[n][j][wh + 19] = mask_1d[n][j][col3 + 5];                                            A_convert[n][j][wh + 20] = mask_1d[n][j][col3 + 6];                                            A_convert[n][j][wh + 21] = mask_1d[n][j][col4];                                            A_convert[n][j][wh + 22] = mask_1d[n][j][col4 + 1];                                            A_convert[n][j][wh + 23] = mask_1d[n][j][col4 + 2];                                            A_convert[n][j][wh + 24] = mask_1d[n][j][col4 + 3];                                            A_convert[n][j][wh + 25] = mask_1d[n][j][col4 + 4];                                            A_convert[n][j][wh + 26] = mask_1d[n][j][col4 + 5];                                            A_convert[n][j][wh + 27] = mask_1d[n][j][col4 + 6];                                            A_convert[n][j][wh + 28] = mask_1d[n][j][col5];                                            A_convert[n][j][wh + 29] = mask_1d[n][j][col5 + 1];                                            A_convert[n][j][wh + 30] = mask_1d[n][j][col5 + 2];                                            A_convert[n][j][wh + 31] = mask_1d[n][j][col5 + 3];                                            A_convert[n][j][wh + 32] = mask_1d[n][j][col5 + 4];                                            A_convert[n][j][wh + 33] = mask_1d[n][j][col5 + 5];                                            A_convert[n][j][wh + 34] = mask_1d[n][j][col5 + 6];                                            A_convert[n][j][wh + 35] = mask_1d[n][j][col6];                                            A_convert[n][j][wh + 36] = mask_1d[n][j][col6 + 1];                                            A_convert[n][j][wh + 37] = mask_1d[n][j][col6 + 2];                                            A_convert[n][j][wh + 38] = mask_1d[n][j][col6 + 3];                                            A_convert[n][j][wh + 39] = mask_1d[n][j][col6 + 4];                                            A_convert[n][j][wh + 40] = mask_1d[n][j][col6 + 5];                                            A_convert[n][j][wh + 41] = mask_1d[n][j][col6 + 6];                                            A_convert[n][j][wh + 42] = mask_1d[n][j][col7];                                            A_convert[n][j][wh + 43] = mask_1d[n][j][col7 + 1];                                            A_convert[n][j][wh + 44] = mask_1d[n][j][col7 + 2];                                            A_convert[n][j][wh + 45] = mask_1d[n][j][col7 + 3];                                            A_convert[n][j][wh + 46] = mask_1d[n][j][col7 + 4];                                            A_convert[n][j][wh + 47] = mask_1d[n][j][col7 + 5];                                            A_convert[n][j][wh + 48] = mask_1d[n][j][col7 + 6];                                        }                                    }                                }                            }                        }                        std::vector<int> C;//存储卷积完了的数字                        //printf("详细的计算信息：");                        for (int k = 0; k < nums; ++k) {                            for (int l = 0; l < 1; ++l) {                                for (int m = 0; m < outm; ++m) {                                    for (int n = 0; n < outm; ++n) {                                        int result_one_position = 0;                                        index_t wh = m * outm * convAw + n * convAw;                                        for (int j = 0; j < convAw; ++j) {                                            result_one_position += A_convert[k][l][wh + j] * kernel_1d[k][l][j];                                        //printf("A_convert[%d][%d][%lld]:%d\t",k,l,wh+j,A_convert[k][l][wh + j]);                                        //printf("kernel_1d[%d][%d][%d]:%d\t",k,l,j,kernel_1d[k][l][j]);                                        }                                        //printf("\n");                                        C.push_back(result_one_position);//input[0]*1*outm*outm                                    }                                }                            }                        }                        //把容器中的数字重组为4维数组                        std::vector<std::vector<std::vector<std::vector<int>>>> mask_conved(nums,                                                                                            std::vector<std::vector<std::vector<int>>>(                                                                                                    1,                                                                                                    std::vector<std::vector<int>>(                                                                                                            outm,                                                                                                            std::vector<int>(                                                                                                                    outm))));                        index_t delta = block_size / 2;//1                        index_t input_height = mask_height + delta * 2;//9                        index_t input_width = mask_width + delta * 2;//9                        index_t height_to_crop = outm - input_height;//0                        index_t width_to_crop = outm - input_width;//0                        if (height_to_crop != 0) {                            printf("height_to_crop !=0");                            for (int k = 0; k < nums; ++k) {                                for (int l = 0; l < 1; ++l) {                                    for (int m = 0; m < outm - height_to_crop + 1; ++m) {                                        printf("\n");                                        for (int n = 0; n < outm; ++n) {                                            mask_conved[k][l][m][n] = (C[k * outm * (outm - height_to_crop)                                                                            + l * outm * (outm - height_to_crop) +                                                                            m * outm + n]==0)? 1:0;                                            printf("%d\t",C[k * outm * (outm - height_to_crop)                                                          + l * outm * (outm - height_to_crop) +                                                          m * outm + n]);                                        }                                    }                                }                            }                        }                        if (width_to_crop != 0) {                            printf("width_to_crop !=0");                            for (int k = 0; k < nums; ++k) {                                for (int l = 0; l < 1; ++l) {                                    for (int m = 0; m < outm; ++m) {                                        printf("\n");                                        for (int n = 0; n < outm - width_to_crop + 1; ++n) {                                            mask_conved[k][l][m][n] =( C[k * outm * (outm - width_to_crop) +                                                                            l * outm * (outm - width_to_crop) +                                                                            m * (outm - width_to_crop) + n]==0)? 1:0;                                            printf("%d\t",C[k * outm * (outm - width_to_crop) +                                                            l * outm * (outm - width_to_crop) +                                                            m * (outm - width_to_crop) + n]);                                        }                                    }                                }                            }                        }                        if ((width_to_crop != 0)&&(height_to_crop!=0)) {                            printf("width_to_crop !=0");                            for (int k = 0; k < nums; ++k) {                                for (int l = 0; l < 1; ++l) {                                    for (int m = 0; m < outm-height_to_crop+1; ++m) {                                        printf("\n");                                        for (int n = 0; n < outm - width_to_crop + 1; ++n) {                                            mask_conved[k][l][m][n] =( C[k * (outm-height_to_crop) * (outm - width_to_crop) +                                                                            l * (outm-height_to_crop) * (outm - width_to_crop) +                                                                            m * (outm - width_to_crop) + n]==0)? 1:0;                                            printf("%d\t",C[k * outm * (outm - width_to_crop) +                                                            l * outm * (outm - width_to_crop) +                                                            m * (outm - width_to_crop) + n]);                                        }                                    }                                }                            }                        }                            printf("width_to_crop和height_to_crop都等于0");                        for (int k = 0; k < nums; ++k) {                            for (int l = 0; l < 1; ++l) {                                for (int m = 0; m < outm; ++m) {                                    printf("\n");                                    for (int n = 0; n < outm; ++n) {                                        mask_conved[k][l][m][n] =(C[k * outm * outm + l * outm * outm + m * outm + n]==0)? 1:0;                                        printf("%d\t",C[k * outm * outm + l * outm * outm + m * outm + n]);                                    }                                    printf("\n");                                }                                printf("\n");                            }                            printf("\n");                        }                        printf("\n");                        //把mask_conved变为一个1D的数组来与indata进行计算                        std::vector<int> mask_conved_1d(nums * channel * height * width);                        printf("mask_conved_1d：\n");                        for (int k = 0; k < nums; ++k) {                            for (int l = 0; l < channel; ++l) {                                for (int m = 0; m < height; ++m) {                                    for (int n = 0; n < width; ++n) {                                        mask_conved_1d[k * channel * height * width                                                       + l * height * width +                                                       m * width + n] = mask_conved[k][0][m][n];                                        printf("%d\t",mask_conved_1d[k * channel * height * width                                                                     + l * height * width +                                                                     m * width + n]);                                    }                                    printf("\n");                                }                                printf("\n");                            }                        }                        DType *input_data=in_data[dropblock::kData].dptr<DType>();                        printf("input_data[0] %d\n",input_data[0]);                        DType *mask_out=out_data[dropblock::kMask].dptr<DType>();                        DType *dropout_out=out_data[dropblock::kOut].dptr<DType>();                        for (index_t i = 0;  i < N; ++i) {                            mask_out[i] = mask_conved_1d[i] * (1.0f / pkeep);                            dropout_out[i] = mask_out[i] * input_data[i];                        }//                        Shape<4> dshape=Shape4(nums,channel,height,width);//                        Tensor<xpu, 4, DType> data_in = in_data[dropblock::kData].get_with_shape<xpu,4,DType>(dshape,s);//                        Tensor<xpu, 4, DType> data_out = out_data[dropblock::kOut].get_with_shape<xpu,4,DType>(dshape,s);                        Assign(data_out, req[dropblock::kOut], data_in );#endif //USE_MKL && _OPENMP                    }                else {                        printf("以上啥都没有发生");                        const TBlob& data = in_data[dropblock::kData];                        if (req[dropblock::kOut] == kWriteTo) {                            mxnet_op::copy(s, out, data);                        } else {                            MXNET_ASSIGN_REQ_SWITCH(req[dropblock::kOut], Req, {                                mxnet_op::Kernel<mxnet_op::op_with_req<mshadow_op::identity, Req>, xpu>::Launch(                                        s, out.Size(), out.dptr<DType>(), data.dptr<DType>());//identity:input==output                            });                        }                    }                }            }            void Backward(const OpContext &ctx,                          const std::vector<TBlob> &out_grad,                          const std::vector<TBlob> &out_data,                          const std::vector<OpReqType> &req,                          const std::vector<TBlob> &in_grad) {                using namespace mshadow;                using namespace mshadow::expr;                Stream<xpu> *s = ctx.get_stream<xpu>();                if (ctx.is_train || mode_ == dropblock::kAlways) {                    if (this->axes_.ndim() != 0 ) {                        const TBlob &gdata = in_grad[dropblock::kData];                        const TBlob &grad = out_grad[dropblock::kOut];                        const TBlob &mask = out_data[dropblock::kMask];//                        if (this->axes_.ndim() == 0) {//                            // standard case for dropout//                            CHECK_EQ(grad.Size(), mask.Size());//                            MXNET_ASSIGN_REQ_SWITCH(req[dropblock::kData], Req, {//                                mxnet_op::Kernel<mxnet_op::op_with_req<mshadow_op::mul, Req>, xpu>::Launch(//                                        s, gdata.Size(), gdata.dptr<DType>(), grad.dptr<DType>(), mask.dptr<DType>());//                            });//                            return;                        if (this->axes_.ndim() == 0) {                        // standard case for dropblock                        CHECK_EQ(grad.Size(), mask.Size());                        MXNET_ASSIGN_REQ_SWITCH(req[dropblock::kData], Req, {                            Kernel<dropblock_backward_kernel<Req>, xpu>::Launch(                                    s, gdata.Size(), gdata.dptr<DType>(), grad.dptr<DType>(), mask.dptr<DType>());                        });                        return;                        }                        // broardcast mul                        TShape new_lshape, new_rshape, new_oshape;                        int ndim = BinaryBroadcastShapeCompact(grad.shape_,                                                               mask.shape_, gdata.shape_,                                                               &new_lshape, &new_rshape, &new_oshape);                        if (!ndim) {                            MXNET_ASSIGN_REQ_SWITCH(req[dropblock::kData], Req, {                                mxnet_op::Kernel<mxnet_op::op_with_req<mshadow_op::mul, Req>, xpu>::Launch(                                        s, gdata.Size(), gdata.dptr<DType>(), grad.dptr<DType>(), mask.dptr<DType>());                            });                        } else {                            BROADCAST_NDIM_SWITCH(ndim, NDim, {                                mshadow::Shape<NDim> oshape = new_oshape.get<NDim>();                                mshadow::Shape<NDim> lstride = mxnet_op::calc_stride(new_lshape.get<NDim>());                                mshadow::Shape<NDim> rstride = mxnet_op::calc_stride(new_rshape.get<NDim>());                                mxnet_op::Kernel<mxnet_op::binary_broadcast_kernel<NDim, DType,                                        mshadow_op::mul>, xpu>::                                template LaunchEx(s, new_oshape.Size(), req[0], lstride, rstride, oshape,                                                  grad.dptr<DType>(), mask.dptr<DType>(), gdata.dptr<DType>());                            });                        }                    }                } else {                    const TBlob& gdata = in_grad[dropblock::kData];                    const TBlob& grad = out_grad[dropblock::kOut];                    if (req[dropblock::kData] == kWriteTo) {                        mxnet_op::copy(s, gdata, grad);                    } else {                        MXNET_ASSIGN_REQ_SWITCH(req[dropblock::kData], Req, {                            mxnet_op::Kernel<mxnet_op::op_with_req<mshadow_op::identity, Req>, xpu>::Launch(                                    s, gdata.Size(), gdata.dptr<DType>(), grad.dptr<DType>());                        });                    }                }            }        private:            /*! \brief Dropout rate (keep when the generated random number is less than this value) */            real_t pkeep_;            /*! \brief Dropout mode */            dropblock::DropblockOpMode mode_;            index_t block_size_;            TShape axes_;        };  // class DropoutOp        template<typename xpu>        void DropblockCompute(const nnvm::NodeAttrs& attrs,                            const OpContext& ctx,                            const std::vector<TBlob>& inputs,                            const std::vector<OpReqType>& req,                            const std::vector<TBlob>& outputs) {            const DropblockParam& param = nnvm::get<DropblockParam>(attrs.parsed);            MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, {                DropblockOp<xpu, DType> op;                op.Init(param);                op.Forward(ctx, inputs, req, outputs);            });        }        template<typename xpu>        void DropblockGradCompute(const nnvm::NodeAttrs& attrs,                                const OpContext& ctx,                                const std::vector<TBlob>& inputs,                                const std::vector<OpReqType>& req,                                const std::vector<TBlob>& outputs) {            const DropblockParam& param = nnvm::get<DropblockParam>(attrs.parsed);            CHECK_EQ(inputs.size(), 2U);            CHECK_EQ(outputs.size(), 1);            CHECK_EQ(req.size(), 1);            std::vector<TBlob> out_grads(2);            std::vector<TBlob> out_data(2);            out_grads[dropblock::kOut] = inputs[0];            out_data[dropblock::kMask] = inputs[1];            MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, {                DropblockOp<xpu, DType> op;                op.Init(param);                op.Backward(ctx, out_grads, out_data, req, outputs);            });        }    }  // namespace op}  // namespace mxnet#endif  // MXNET_OPERATOR_NN_DROPOUT_INL_H_